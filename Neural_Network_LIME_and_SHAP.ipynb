{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM9UXJDYv6ND2wLjbqorxYx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehas1/Statistical-Bias-in-ML/blob/main/Neural_Network_LIME_and_SHAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hN9xGYK4J_s"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import shap\n",
        "shap.initjs()\n",
        "! pip install lime\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_filtered = pd.read_csv('cox-violent-parsed_filt.csv')\n"
      ],
      "metadata": {
        "id": "pCBjc1so4bEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df_filtered[[\"sex\", 'age', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree']]\n",
        "Y = df_filtered['is_recid']\n",
        "\n",
        "categorical_features = ['sex', 'race', 'c_charge_degree']\n",
        "numerical_features = ['age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n",
        "\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_features)"
      ],
      "metadata": {
        "id": "hA_sDpoG5LNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Create directory for saved models if it doesn't exist\n",
        "if not os.path.exists('recidivism_model'):\n",
        "    os.makedirs('recidivism_model')\n",
        "\n",
        "# Assume df_filtered is already loaded\n",
        "\n",
        "# Define features and target\n",
        "X = df_filtered[[\"sex\", 'age', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree']]\n",
        "Y = df_filtered['is_recid']\n",
        "\n",
        "categorical_features = ['sex', 'race', 'c_charge_degree']\n",
        "numerical_features = ['age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_features)\n",
        "\n",
        "# Normalize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_encoded[numerical_features] = scaler.fit_transform(X_encoded[numerical_features])\n",
        "\n",
        "# Convert Y to categorical (binary classification)\n",
        "Y = Y.astype(int)  # Ensure it's integer (0 or 1)\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=42)\n",
        "y_train = y_train.clip(0, 1)\n",
        "\n",
        "# Define Neural Network Model\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.02)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    # Second block\n",
        "    Dense(64, activation='relu', kernel_regularizer=l2(0.02)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "\n",
        "    # Third block\n",
        "    Dense(32, activation='relu', kernel_regularizer=l2(0.02)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    # Output layer\n",
        "    Dense(1, activation='sigmoid', kernel_regularizer=l2(0.02))\n",
        "])\n",
        "\n",
        "# Compile Model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Early Stopping\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    restore_best_weights=True,\n",
        "    mode='min'\n",
        ")\n",
        "\n",
        "# Learning rate reduction on plateau\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',\n",
        "    factor=0.5,\n",
        "    patience=2,\n",
        "    min_lr=0.00001,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Train Model\n",
        "history = model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_data=(X_test, y_test),\n",
        "    epochs=15,\n",
        "    batch_size=64,\n",
        "    callbacks=[early_stopping, reduce_lr]\n",
        ")\n",
        "\n",
        "# Extract training history\n",
        "history_dict = history.history\n",
        "\n",
        "# Evaluate Model\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# Create plots for loss and accuracy to check for overfitting\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_dict['loss'], label='Training Loss', linestyle='-', marker='o', color='blue')\n",
        "plt.plot(history_dict['val_loss'], label='Validation Loss', linestyle='-', marker='o', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss (Binary Crossentropy)')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_dict['accuracy'], label='Training Accuracy', linestyle='-', marker='o', color='blue')\n",
        "plt.plot(history_dict['val_accuracy'], label='Validation Accuracy', linestyle='-', marker='o', color='green')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Show plots\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save the trained model\n",
        "model.save('recidivism_model/neural_network_model.keras')\n",
        "print(\"Model saved to 'recidivism_model/neural_network_model.keras'\")\n",
        "\n",
        "# Save the scaler for future preprocessing\n",
        "joblib.dump(scaler, 'recidivism_model/scaler.save')\n",
        "print(\"Scaler saved to 'recidivism_model/scaler.save'\")\n",
        "\n",
        "# Save feature names for reference\n",
        "feature_names = {\n",
        "    'categorical_features': categorical_features,\n",
        "    'numerical_features': numerical_features\n",
        "}\n",
        "joblib.dump(feature_names, 'recidivism_model/feature_names.save')\n",
        "print(\"Feature names saved to 'recidivism_model/feature_names.save'\")\n",
        "\n",
        "print(\"\\nTo load the recidivism neural network model in another notebook:\")\n",
        "print(\"\"\"\n",
        "# Load the saved model and components\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "\n",
        "model = load_model('recidivism_model/neural_network_model')\n",
        "scaler = joblib.load('recidivism_model/scaler.save')\n",
        "feature_names = joblib.load('recidivism_model/feature_names.save')\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "JPXbkBc48hiE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP"
      ],
      "metadata": {
        "id": "BlVNa3MQ5OlS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Individual Plots"
      ],
      "metadata": {
        "id": "4wDBWpzi5Vcm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Convert background data to float explicitly\n",
        "X_background = X_train.iloc[:50].astype(np.float64).values\n",
        "\n",
        "# Define a prediction function that takes a 2D numpy array and returns predictions.\n",
        "def f(X):\n",
        "    return model.predict(X).flatten()\n",
        "\n",
        "# Create the SHAP KernelExplainer using the numeric background data.\n",
        "explainer = shap.KernelExplainer(f, X_background)\n",
        "\n",
        "# Choose a sample index to explain (ensure this index exists in X_test).\n",
        "sample_index = 299\n",
        "\n",
        "# Get the sample as a 2D array and convert to float.\n",
        "X_sample = X_test.iloc[[sample_index]].astype(np.float64).values\n",
        "\n",
        "# Compute SHAP values for the chosen sample.\n",
        "shap_values = explainer.shap_values(X_sample, nsamples=500)\n",
        "\n",
        "# Initialize JS visualization (if using a Jupyter notebook)\n",
        "shap.initjs()\n",
        "\n",
        "# Display the force plot.\n",
        "# Note: We're using the original DataFrame row (which has column names) for display.\n",
        "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[sample_index])\n"
      ],
      "metadata": {
        "id": "YfeZa_v15aUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Global Plots"
      ],
      "metadata": {
        "id": "sd-USCJM5cdC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "import numpy as np\n",
        "\n",
        "# Initialize SHAP's JS visualization (run this once in the notebook)\n",
        "shap.initjs()\n",
        "# Select multiple samples (10 samples, for instance)\n",
        "samples = X_test.iloc[:10]  # shape: (10, 540)\n",
        "samples_array = samples.to_numpy().astype(np.float32)\n",
        "\n",
        "# Compute SHAP values for each sample individually and store them in a list.\n",
        "shap_values_list = []\n",
        "for i in range(samples_array.shape[0]):\n",
        "    sample_i = samples_array[i:i+1, :]  # shape (1, 540)\n",
        "    # Compute SHAP values for the single sample.\n",
        "    # DeepExplainer returns a list (one entry per model output), so we take the first element.\n",
        "    shap_value_i = explainer.shap_values(sample_i)[0]\n",
        "\n",
        "    # For a single sample we observed a shape of (540, 1) so we transpose it to get (1, 540)\n",
        "    if shap_value_i.shape[0] == sample_i.shape[1]:\n",
        "        shap_value_i = shap_value_i.T\n",
        "    shap_values_list.append(shap_value_i)\n",
        "\n",
        "# Stack the individual arrays to form one array for all samples.\n",
        "# Expected shape: (10, 540)\n",
        "shap_values_multi_corrected = np.vstack(shap_values_list)\n",
        "\n",
        "print(\"Corrected SHAP values shape:\", shap_values_multi_corrected.shape)\n",
        "# Now create a summary plot\n",
        "shap.summary_plot(shap_values_multi_corrected, samples)\n"
      ],
      "metadata": {
        "id": "m48EqTdf5ejb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LIME"
      ],
      "metadata": {
        "id": "k2YCQfm8-e3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import os\n",
        "import shap\n",
        "\n",
        "# Create directory for saved models if it doesn't exist\n",
        "if not os.path.exists('recidivism_model'):\n",
        "    os.makedirs('recidivism_model')\n",
        "\n",
        "# Check if model and components exist\n",
        "model_path = 'recidivism_model/neural_network_model.keras'\n",
        "scaler_path = 'recidivism_model/scaler.save'\n",
        "feature_names_path = 'recidivism_model/feature_names.save'\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    raise FileNotFoundError(f\"Model file not found at {model_path}. Please run recidivism_neural_network.py first.\")\n",
        "\n",
        "if not os.path.exists(scaler_path):\n",
        "    raise FileNotFoundError(f\"Scaler file not found at {scaler_path}. Please run recidivism_neural_network.py first.\")\n",
        "\n",
        "if not os.path.exists(feature_names_path):\n",
        "    raise FileNotFoundError(f\"Feature names file not found at {feature_names_path}. Please run recidivism_neural_network.py first.\")\n",
        "\n",
        "# Load the saved model and components\n",
        "model = load_model(model_path)\n",
        "scaler = joblib.load(scaler_path)\n",
        "feature_names = joblib.load(feature_names_path)\n",
        "\n",
        "# Load and preprocess the data\n",
        "data_path = 'cox-violent-parsed_filt.csv'\n",
        "if not os.path.exists(data_path):\n",
        "    raise FileNotFoundError(f\"Data file not found at {data_path}. Please ensure the data file is in the correct location.\")\n",
        "\n",
        "df_filtered = pd.read_csv(data_path)\n",
        "\n",
        "# Define features and target (same as in the original script)\n",
        "X = df_filtered[[\"sex\", 'age', 'race', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count', 'c_charge_degree']]\n",
        "Y = df_filtered['is_recid']\n",
        "\n",
        "categorical_features = feature_names['categorical_features']\n",
        "numerical_features = feature_names['numerical_features']\n",
        "\n",
        "# One-hot encode categorical features\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_features)\n",
        "\n",
        "# Store original X for reference\n",
        "X_test_original = X.copy()\n",
        "\n",
        "# Normalize numerical features\n",
        "X_encoded[numerical_features] = scaler.transform(X_encoded[numerical_features])\n",
        "\n",
        "# Train/Test Split (use same random_state as original)\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# --------------------------\n",
        "# LIME Explanation Setup\n",
        "# --------------------------\n",
        "def predict_fn(data):\n",
        "    data = data.astype(np.float32)\n",
        "    preds = model.predict(data)  # shape (n_samples, 1)\n",
        "    return np.hstack([1 - preds, preds])\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.to_numpy(),\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    class_names=[\"Not Recid\", \"Recid\"],\n",
        "    mode=\"classification\",\n",
        "    discretize_continuous=False  # show actual scaled values for numeric features\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Helper Functions for Mapping & Plotting\n",
        "# --------------------------\n",
        "def extract_feature_name(feature_str):\n",
        "    \"\"\"\n",
        "    Extract the pure feature name from a LIME explanation string.\n",
        "    E.g., \"age <= 0.5\" becomes \"age\", \"sex_F = 1\" becomes \"sex_F\".\n",
        "    \"\"\"\n",
        "    if isinstance(feature_str, str):\n",
        "        for delim in [' <= ', ' > ', ' = ']:\n",
        "            if delim in feature_str:\n",
        "                return feature_str.split(delim)[0]\n",
        "        return feature_str\n",
        "    return str(feature_str)\n",
        "\n",
        "def get_feature_value(feature_str, instance_index):\n",
        "    \"\"\"\n",
        "    For a given explanation feature, return the original instance value.\n",
        "    For one-hot encoded features, map back to the original categorical value.\n",
        "    For numeric features, return the raw (unscaled) numeric value from X_test_original.\n",
        "    \"\"\"\n",
        "    pure_feat = extract_feature_name(feature_str)\n",
        "\n",
        "    # Check if the feature is a one-hot encoded categorical column.\n",
        "    for cat in categorical_features:\n",
        "        prefix = cat + \"_\"\n",
        "        if pure_feat.startswith(prefix):\n",
        "            # active_cat is the category represented by this column.\n",
        "            active_cat = pure_feat[len(prefix):]\n",
        "            orig_val = X_test_original.loc[instance_index, cat]\n",
        "            return f\"{active_cat} (actual: {orig_val})\"\n",
        "\n",
        "    # Otherwise, if the pure feature is a numeric feature,\n",
        "    # use the original unscaled value from X_test_original.\n",
        "    if pure_feat in numerical_features:\n",
        "        num_val = X_test_original.loc[instance_index, pure_feat]\n",
        "        return f\"{num_val}\"\n",
        "\n",
        "    # Otherwise, return the feature string itself.\n",
        "    return feature_str\n",
        "\n",
        "def plot_lime_explanation(exp_data, instance_index, title):\n",
        "    \"\"\"\n",
        "    Plot a horizontal bar chart for the given LIME explanation.\n",
        "    Customizes the label to show the instance's value:\n",
        "      - For numeric features: the raw number.\n",
        "      - For one-hot categorical features: the mapped original category.\n",
        "    \"\"\"\n",
        "    # exp_data is a list of (feature, weight) tuples.\n",
        "    sorted_exp = sorted(exp_data, key=lambda x: abs(x[1]), reverse=True)\n",
        "    features = [f for f, w in sorted_exp]\n",
        "    weights = [w for f, w in sorted_exp]\n",
        "\n",
        "    # Build custom labels by appending the instance's value.\n",
        "    custom_labels = [f\"{f} (value: {get_feature_value(f, instance_index)})\" for f in features]\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.barh(custom_labels, weights, color='skyblue')\n",
        "    plt.xlabel(\"LIME Weight\")\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()  # highest weight at the top\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# Generate LIME Explanations and Visuals for 5 Random Test Instances\n",
        "# --------------------------\n",
        "print(\"\\nGenerating LIME explanations for 5 random test instances...\")\n",
        "instances = X_test.sample(5, random_state=42)\n",
        "for idx, row in instances.iterrows():\n",
        "    explanation = explainer.explain_instance(\n",
        "        data_row=row.values,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    # Determine predicted label\n",
        "    pred_probs = predict_fn(row.values.reshape(1, -1))\n",
        "    pred_label = np.argmax(pred_probs[0])\n",
        "    available_labels = list(explanation.as_map().keys())\n",
        "    if pred_label not in available_labels:\n",
        "        pred_label = available_labels[0]\n",
        "\n",
        "    print(f\"\\n--- LIME Explanation for Test Instance Index: {idx} ---\")\n",
        "    exp_list = explanation.as_list(label=pred_label)\n",
        "    for feature, weight in exp_list:\n",
        "        print(f\"{feature} => {weight:.4f}\")\n",
        "\n",
        "    # Plot custom bar chart with values mapped to the original data\n",
        "    plot_lime_explanation(exp_list, idx, f\"LIME Explanation for Test Instance {idx}\")"
      ],
      "metadata": {
        "id": "XHy0utCG-hCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import lime.lime_tabular\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --------------------------\n",
        "# Data Preparation\n",
        "# --------------------------\n",
        "# Assume df_filtered is already loaded\n",
        "\n",
        "# Define features and target (original DataFrame)\n",
        "X = df_filtered[[\"sex\", \"age\", \"race\", \"juv_fel_count\", \"juv_misd_count\",\n",
        "                 \"juv_other_count\", \"priors_count\", \"c_charge_degree\"]]\n",
        "Y = df_filtered[\"is_recid\"]\n",
        "\n",
        "# Define which columns are categorical and numeric\n",
        "categorical_features = ['sex', 'race', 'c_charge_degree']\n",
        "numerical_features = ['age', 'juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count']\n",
        "\n",
        "# One-hot encode categorical features (this is what the model uses)\n",
        "X_encoded = pd.get_dummies(X, columns=categorical_features)\n",
        "\n",
        "# Normalize numeric features in the encoded DataFrame\n",
        "scaler = StandardScaler()\n",
        "X_encoded[numerical_features] = scaler.fit_transform(X_encoded[numerical_features])\n",
        "X_encoded = X_encoded.astype(np.float32)\n",
        "Y = Y.astype(int)\n",
        "\n",
        "# Train/test split on the encoded data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_encoded, Y, test_size=0.2, random_state=42)\n",
        "y_train = y_train.clip(0, 1)\n",
        "\n",
        "# Also, keep a copy of the original data (before one-hot encoding) for mapping values\n",
        "X_test_original = X.loc[X_test.index]\n",
        "\n",
        "# --------------------------\n",
        "# Neural Network Model\n",
        "# --------------------------\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(16, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(8, activation='relu'),\n",
        "    BatchNormalization(),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=18, batch_size=32)\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
        "\n",
        "# --------------------------\n",
        "# LIME Explanation Setup\n",
        "# --------------------------\n",
        "def predict_fn(data):\n",
        "    data = data.astype(np.float32)\n",
        "    preds = model.predict(data)  # shape (n_samples, 1)\n",
        "    return np.hstack([1 - preds, preds])\n",
        "\n",
        "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "    training_data=X_train.to_numpy(),\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    class_names=[\"Not Recid\", \"Recid\"],\n",
        "    mode=\"classification\",\n",
        "    discretize_continuous=False  # show actual (scaled) numeric values\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# Helper Functions for Mapping & Plotting\n",
        "# --------------------------\n",
        "def extract_feature_name(feature_str):\n",
        "    \"\"\"\n",
        "    Extract the pure feature name from a LIME explanation string.\n",
        "    E.g., \"age <= 0.5\" becomes \"age\", \"sex_F = 1\" becomes \"sex_F\".\n",
        "    \"\"\"\n",
        "    if isinstance(feature_str, str):\n",
        "        for delim in [' <= ', ' > ', ' = ']:\n",
        "            if delim in feature_str:\n",
        "                return feature_str.split(delim)[0]\n",
        "        return feature_str\n",
        "    return str(feature_str)\n",
        "\n",
        "def get_feature_value(feature_str, instance_index):\n",
        "    \"\"\"\n",
        "    For a given explanation feature, return the original instance value.\n",
        "    For one-hot encoded features, map back to the original categorical value.\n",
        "    For numeric features, return the raw (unscaled) numeric value from X_test_original.\n",
        "    \"\"\"\n",
        "    pure_feat = extract_feature_name(feature_str)\n",
        "\n",
        "    # Check if the feature is a one-hot encoded categorical column.\n",
        "    for cat in categorical_features:\n",
        "        prefix = cat + \"_\"\n",
        "        if pure_feat.startswith(prefix):\n",
        "            # active_cat is the category represented by this column.\n",
        "            active_cat = pure_feat[len(prefix):]\n",
        "            orig_val = X_test_original.loc[instance_index, cat]\n",
        "            return f\"{active_cat} (actual: {orig_val})\"\n",
        "\n",
        "    # Otherwise, if the pure feature is a numeric feature,\n",
        "    # use the original unscaled value from X_test_original.\n",
        "    if pure_feat in numerical_features:\n",
        "        num_val = X_test_original.loc[instance_index, pure_feat]\n",
        "        return f\"{num_val}\"\n",
        "\n",
        "    return feature_str\n",
        "\n",
        "def plot_lime_explanation_with_values(exp_data, instance_index, title):\n",
        "    \"\"\"\n",
        "    Plot a horizontal bar chart for the LIME explanation exp_data.\n",
        "    Custom labels include the feature name and the actual dataset value.\n",
        "    \"\"\"\n",
        "    # exp_data is a list of (feature, weight) tuples.\n",
        "    sorted_exp = sorted(exp_data, key=lambda x: abs(x[1]), reverse=True)\n",
        "    features = [f for f, w in sorted_exp]\n",
        "    weights = [w for f, w in sorted_exp]\n",
        "\n",
        "    # Build custom labels by appending the instance's value from X_test_original.\n",
        "    custom_labels = [f\"{f} (value: {get_feature_value(f, instance_index)})\" for f in features]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh(custom_labels, weights, color='skyblue')\n",
        "    plt.xlabel(\"LIME Weight\")\n",
        "    plt.title(title)\n",
        "    plt.gca().invert_yaxis()  # highest weight on top\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# --------------------------\n",
        "# Generate LIME Explanations and Visuals for 5 Random Test Instances\n",
        "# --------------------------\n",
        "instances = X_test.sample(5, random_state=42)\n",
        "for idx, row in instances.iterrows():\n",
        "    # Generate LIME explanation for this instance\n",
        "    explanation = explainer.explain_instance(\n",
        "        data_row=row.values,\n",
        "        predict_fn=predict_fn,\n",
        "        num_features=10\n",
        "    )\n",
        "\n",
        "    # Determine the predicted label\n",
        "    pred_probs = predict_fn(row.values.reshape(1, -1))\n",
        "    pred_label = np.argmax(pred_probs[0])\n",
        "    available_labels = list(explanation.as_map().keys())\n",
        "    if pred_label not in available_labels:\n",
        "        pred_label = available_labels[0]\n",
        "\n",
        "    print(f\"\\n--- LIME Explanation for Test Instance Index: {idx} ---\")\n",
        "    exp_list = explanation.as_list(label=pred_label)\n",
        "    for feature, weight in exp_list:\n",
        "        print(f\"{feature} => {weight:.4f}\")\n",
        "\n",
        "    # Show the original LIME interactive visualization (if in Jupyter Notebook)\n",
        "    explanation.show_in_notebook(show_table=True, show_all=False)\n",
        "\n",
        "    # Plot custom bar chart with the actual dataset values in the labels\n",
        "    plot_lime_explanation_with_values(exp_list, idx, f\"LIME Explanation for Test Instance {idx}\")\n"
      ],
      "metadata": {
        "id": "QwkcdP8S-3kt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}