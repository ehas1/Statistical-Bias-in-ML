{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ehas1/Statistical-Bias-in-ML/blob/main/Decision_Tree_%26_Neural_Net.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Xelss9NurGK"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "# [add code here]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define functions\n",
        "\n",
        "def load_data(url=\"add default value here\") -> pd.DataFrame:\n",
        "  \"\"\"Download COMPAS data set from Github to current directory. (Add a copy of\n",
        "  the file to your repository and download it from there.) Then load file into\n",
        "  RAM.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return data\n",
        "\n",
        "\n",
        "def preprocess_data(data: pd.DataFrame, random_state: int = 42) -> tuple:\n",
        "  \"\"\"Preprocess COMPAS data set. Include feature grabbing, string parsing, 1-hot\n",
        "  encoding, train-test split and feature normalization.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  train_data = [X_train_scaled, y_train]\n",
        "  test_data = [X_test_scaled, y_test]\n",
        "\n",
        "  return train_data, test_data\n",
        "\n",
        "\n",
        "def train_decision_tree(train_data: list, params: dict = None) -> object:\n",
        "  \"\"\"Decision tree training on full training data set (i.e., without holdout set\n",
        "  or crossvalidation. (I didn't know how to type the output. Is it an sklearn\n",
        "  model?)\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return model\n",
        "\n",
        "def tree_viz(model: object) -> plt.figure:\n",
        "  \"\"\"Create a visualization of a decision tree.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "def test_model(model: object, data: list) -> dict:\n",
        "  \"\"\"Test a given model (is it a classifier or a regressor?). Include all\n",
        "  metrics that you want to use in later analyses. This may include accuracy,\n",
        "  loss, TPR, FPR, etc. Return them as a dictionary.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return evals\n",
        "\n",
        "\n",
        "def plot_evals(evals: dict) -> plt.figure:\n",
        "  \"\"\"Show training and test accuracy as a function of number of epochs. Maybe\n",
        "  include other metrics too if you have done so in your original notebook.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "def train_decision_tree_with_crossvalidation(train_data: list,\n",
        "  n_fold: int = 5, params: dict = None, shuffle=True, random_state=42) -> dict:\n",
        "  \"\"\"Split training data into folds, loops over held-out folds for training\n",
        "  rounds, calculates and stores performance metrics for each run of the loop as\n",
        "  a function of the number of epochs. Return a dictionary of validation evals.\n",
        "  Either save the models to file or return them with the output.\"\"\"\n",
        "\n",
        "  kf = KFold(n_splits=n_fold, shuffle=shuffle, random_state=random_state)\n",
        "  models = [\"\" for i in range(n_fold)]\n",
        "  evals = [\"\" for i in range(n_fold)]\n",
        "\n",
        "  for i, (train_set, val_set) in enumerate(kf.split(train_data)):\n",
        "\n",
        "    models[i] = train_decision_tree(train_set, params=params)\n",
        "    cv_evals[i] = {\"train_evals\": test_model(model, train_set),\n",
        "      \"validation_evals\": test_model(model, val_set)}\n",
        "\n",
        "  return cv_evals, models\n",
        "\n",
        "\n",
        "def plot_cv_evals(cv_evals: list) -> plt.figure:\n",
        "  \"\"\"Show training and test accuracy as a function of number of epochs. Instead\n",
        "  of showing 5 separate plots for 5-fold crossvalidation, just show the mean as\n",
        "  a line in the foreground and the envelope from min to max as a shaded area\n",
        "  in the background.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return fig\n",
        "\n",
        "\n",
        "def optimize_tree_depth(train_data: list, min_td: int = 2, max_td: int = 6,\n",
        "  n_fold: int = 5, params: dict = None) -> dict:\n",
        "  \"\"\"Perform crossvalidated training pipeline for several values of tree_depth.\n",
        "  Get training evals for all values. Return a dictionary of evals.\n",
        "  Either save the models to file or return them with the output.\"\"\"\n",
        "\n",
        "  optimization_evals ={}\n",
        "\n",
        "  for tree_depth in range(min_td, max_td+1):\n",
        "    cv_evals, models = train_decision_tree_with_crossvalidation(train_data,\n",
        "      n_fold=n_fold, params=params)\n",
        "    optimization_evals[tree_depth] = cv_evals\n",
        "\n",
        "  return optimization_evals\n",
        "\n",
        "\n",
        "def plot_optimization_evals(cv_evals: dict) -> plt.figure:\n",
        "  \"\"\"Create plots that show test and validation accuracy as a function of tree\n",
        "  depth. Show results for different numbers of epochs as different lines.\"\"\"\n",
        "\n",
        "  # [add code here]\n",
        "\n",
        "  return fig\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VrzhH1d0uvEK",
        "outputId": "446c996a-8ee0-4bf6-cdc6-0937e1089961"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "expected ':' (ipython-input-2-3176591.py, line 41)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2-3176591.py\"\u001b[0;36m, line \u001b[0;32m41\u001b[0m\n\u001b[0;31m    n_fold: int = 5, params: dict = None) -> ???:\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m expected ':'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# DATA\n",
        "\n",
        "data = load_data()\n",
        "train_data, test_data = preprocess_data(data)"
      ],
      "metadata": {
        "id": "FqA8OJPLklWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DECISION TREE\n",
        "\n",
        "# set parameters\n",
        "params = ...\n",
        "\n",
        "# run hyperparameter optimization for tree-depth using 5-fold crossvalidation\n",
        "min_td, max_td = 2, 6\n",
        "evals = optimize_tree_depth(train_data, min_td=min_td, max_td=max_td, n_fold=5,\n",
        "  params=params)\n",
        "fig = plot_optimization_evals(evals)\n",
        "fig.show()\n",
        "\n",
        "# set hyperparameters to best found value(i.e., tree depth)\n",
        "params[...] = ...\n",
        "\n",
        "# check crossvalidation results for optimal hyperparamters (i.e., optimal tree depth)\n",
        "fig2 = plot_crossvalidation_evals(evals)\n",
        "fig2.show()\n",
        "\n",
        "# train with optimal hyperparameters (i.e., optimal tree depth)\n",
        "model_dt = train_decision_tree(train_data, params=params)\n",
        "evals = test_model(model_dt, test_data)\n",
        "\n",
        "# show final tree\n",
        "fig3 = tree_viz(model)\n",
        "fig3.show()\n",
        "\n",
        "# show evals for final tree\n",
        "fig4 = plot_evals(evals)\n",
        "fig4.show()\n",
        "\n",
        "# save final model\n",
        "# [add code here]"
      ],
      "metadata": {
        "id": "g1SezabibVP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NEURAL NETWORK\n",
        "\n",
        "# set parameters\n",
        "params = ...\n",
        "\n",
        "# run hyperparameter optimization using 5-fold crossvalidation\n",
        "\n",
        "# [add code here]\n",
        "\n",
        "# set hyperparameters\n",
        "params[...] = ...\n",
        "\n",
        "# check crossvalidation results for optimal hyperparameter values\n",
        "fig2 = plot_crossvalidation_evals(evals)\n",
        "fig2.show()\n",
        "\n",
        "# train with optimal hyperparameters on full training set\n",
        "model_nn = train_neural_network(train_data)\n",
        "evals = test_model(model_nn, test_data)\n",
        "fig3 = plot_evals(evals)\n",
        "fig3.show()\n",
        "\n",
        "# save final model\n",
        "# [add code here]"
      ],
      "metadata": {
        "id": "H4-JKu8Pkwp6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}